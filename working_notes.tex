\documentclass[12pt]{article}

\usepackage{hyperref}

\newcommand{\link}[2]{\href{#1}{#2}}


\begin{document}

\title{Notes on neural networks}
\author{Michael Nielsen\thanks{Email: mn@michaelnielsen.org}$^{,}$\thanks{Web: http://michaelnielsen.org/ddi}}

\maketitle

\textbf{Working notes, by Michael Nielsen:} These are rough working
notes, written as part of my study of neural networks.  Note that they
really are \emph{rough}, and I've made no attempt to clean them up.
There contain misunderstandings, misinterpretations, omissions, and
outright errors.  I make no apology for this, nor do I plan to clean
them up.  As such, I don't advise others to read the notes, and
certainly not to rely on them!

\textbf{Core questions:} There is a practical, narrow question: what
are the most significant results about deep learning and neural
networks?  And then there is the broader question: how to build a
mind?  My reading will address both questions.

\textbf{Recurrent neural networks (RNN):} According to Wikipedia, RNNs
have achieved the best results to date on handwriting recognition.  A
core question is obviously: what are the respective advantages of RNNs
and feedforward networks?  Are there important problems for which one
or the other is particularly preferable?  Why?  What I've read about
these questions seems opaque.

\textbf{Williams and Zipser (1989) - a gradient-based learning method
  for recurrent neural networks:}
\link{http://scholar.google.ca/scholar?cluster=1352799553544912946\&hl=en\&as\_sdt=0,5}{(link)}
  
  Makes the claim that feedforward networks don't have the "ability to
  store information for later use".  It'd be nice to understand what
  that means.  Obviously there's a trivial sense in which feedforward
  networks can store information based on training data.  Claims that
  backprop requires lots of memory when used with large amounts of
  training data.  I don't understand this claim.
  
  Their model of recurrent neural networks is interesting.  Basically,
  we have a set of neurons, each with an output.  And we have a set of
  inputs to the network.  There is a weight between every pair of
  neurons, and from each input to each neuron.  To compute a neuron's
  output at time $t+1$ we compute the weighted sum of the inputs and
  the outputs at time $t$, and apply the appropriate nonlinear
  function (sigmoid, or whatever).  Note that in order for this
  description to make sense we must specify the behaviour of the
  external inputs over time.  We can incorporate a bias by having an
  external input which is always $1$.
  
  So a recurrent neural network is just like a feedforward network,
  with a weight constraint: the weights in each layer are the same,
  over and over again.  Also, the inputs must be input to every layer
  in the network.
  
  Williams and Zipser take as their supervised training task the goal
  of getting neuron outputs to match certain desired training values
  at certain times.  For instance, you could define a two-neuron
  network that will \emph{eventually} produce the XOR of the inputs.
  
  They define the total error to be the sum over squares of the errors
  in individual neuron outputs.  And we can then do ordinary gradient
  descent with that error function.  They derive a simple dynamical
  system to describe how to improve the weights in the network using
  gradient descent.

  The above algorithm assumes that the weights in the network remain
  constant for all time.  Williams and Zipser then modify the learning
  algorithm, allowing it to change the weights at each step.  The idea
  is simply to compute the total error at any \emph{given} time, and then
  to use gradient descent with that error function to update the
  weights.  (Similar to online learning in feedforward networks.)
  
  Williams and Zipser describe a method of \emph{teacher-forcing},
  modifying the neural network by replacing the output of certain
  neurons by the \emph{desired} output, for training purposes in later
  steps.
  
  Unfortunately, it is still really unclear to me \emph{why} one would
  wish to use recurrent neural networks.  Williams and Zipser describe
  a number of examples, but they don't seem terribly compelling to me.
  
  The algorithm in which the weights can change seems very
  non-physiological to me --- it verges on being an unmotivated
  statistical model.  (I doubt that the weights in the brain swing
  around wildly, but I'll bet that the weights found by this algorithm
  can swing around wildly.)  The algorithm in which the weights are
  fixed seems more biological.
  
  Note that Williams and Zipser \emph{do not} offer any analysis of
  running time for their algorithms, or an understanding of when it is
  likely to work well, and when it is not.  It's very much in the
  empirical let's-see-how-this-works style adopted through much of the
  neural networks literature.
  
  Summing up: the recurrent neural network works by, at each step,
  computing the sigmoid function of the weighted sum of the inputs and
  the previous step's outputs.  Training means specifying a set of
  desired outputs at particular times, and adapting the weights at
  each time-step.  Training works by specifying an error function at
  any given time step, computing the gradient, and updating the
  weights appropriately.

\textbf{Compiling to neural networks:} One idea that may be
  interesting is to create compilers which would translate a program
  written in a conventional programming language into a neural
  network.  I'd be especially interested in seeing how this works for
  AI workhorses such as Prolog.  What could we learn from such a
  procedure?  (1) Perhaps we could figure out how to link up multiple
  neural modules, with one or more of the modules coming from the
  compiler? (2) Maybe we could use a learning technique to further
  improve the performance of the compiled network.  Googling doesn't
  reveal a whole lot, although I did find a paper by
  \link{http://scholar.google.ca/scholar?cluster=10518384657895134615\&hl=en\&as\_sdt=0,5}{Thrun}
  where he discusses decompiling, i.e., extracting rules from a neural
  network.  Thrun uses a technique he calls validity-interval
  analysis, basically propagating intervals for inputs and outputs
  forwards and backwards through a network.

\textbf{Softmax function:} Suppose $q_j$ is some set of values.  Then
  we define the softmax function by:

$$p_j \equiv \exp(q_j)/\sum_k \exp(q_j).$$
  
This is a probability distribution, which preserves the order of the
original values.  You can, for example, take the softmax in the final
layer of a neural network, taking the weighted sum of inputs as the
$q_j$ values, and then applying the softmax.  The output from the
network can then be interpreted as a probability distribution.

\textbf{Baldi and Hornik, 1989 - characterizes linear autoencoders}:
\link{http://scholar.google.ca/scholar?cluster=11637720331851320383&hl=en&as_sdt=0,5}{(link)}
We have a three-layer network, and the output is related to the input
by $x \rightarrow ABx$, where $B$ describes the first layer of
weights, and $A$ the second layer.  The goal is to find weight
matrices $A$ and $B$ to minimize:
\begin{eqnarray}
\sum_x \|x-ABx\|^2.
\end{eqnarray}
The challenge is that the hidden layer has a \emph{smaller} number $h$
of neurons than the input layer (which is, of course, of the same size
as the output layer)\footnote{It's not quite clear to me what $h$
  should parameterize.  I'll use it to parameterize the number of
  dimensions in the vector space representing outputs from the hidden
  units.  It seems likely that it'd be better to write $2^h$, but I'll
  ignore that.}.  Let me try an attack on this without reading the
paper.  That sum above is just:
\begin{eqnarray}
\mbox{tr}((I-AB)^2 \Sigma),
\end{eqnarray}
where $\Sigma \equiv \sum_x x x^T$.  To minimize this what we want to
do is obvious (and easily proven): we'll choose $A$ and $B$ so that
$AB$ is a $h$-dimensional projector onto the span of the eignenvectors
of $\Sigma$ with the $h$ largest eigenvalues.  Let $P(\Sigma, h)$
denote such a projector, so:
\begin{eqnarray}
AB = P(\Sigma, h).
\end{eqnarray}
We can easily characterize such $A$ and $B$.  $A$ should take the
space $P(\Sigma, h)$ into the space spanned by the outputs from the
hidden units, and $B$ should then undo that transformation.  There is
an orthogonal freedom inbetween time, and a possible freedom in
$P(\Sigma, h)$.  This completely characterizes $A$ and $B$.

Summing up, in a linear neural network, \emph{a linear autoencoder is
  just doing principal components analysis}.  So \emph{a non-linear
  autoencoder can be thought of as a non-linear generalization of
  PCA}.  That's a useful fact to remember.  Examination of the
remainder of the paper suggests that these are the key facts.

\textbf{Thinking geometrically:} Suppose we're asked to tell the
difference between pictures of a human face, and pictures of a
giraffe.  We can represent the pictures as points $x$ in a very
high-dimensional space.  And so our task is to divide that space up
into two parts: one is classified as giraffe, the other as human face.
(Maybe it should be three parts: the thrid part would be: neither face
nor giraffe).  And so what we really want is algorithms for dividing
up that space.  In some sense we're interested in understanding the
space of all such algorithms. 

It'd be interesting to lay out all the different curlicues to thinking
in this way: the opportunities, and the pitfalls.  There are at least
three broad approaches: (1) the \emph{pure geometric approach}, based
on finding mathematical structures to divide the space; (2) the
\emph{biological approach}, where we try to figure out how we do it;
and (3) the \emph{kludge approach}, where we simply try lots of ideas,
and pile them up on top of one another.  That's a pretty rough
division, but seems like a good starting point for thought.  My bet is
that progress comes from playing these ideas off against one another.

\textbf{Finding linear approximations (PCA):} It'll be useful to
review PCA here.  Suppose we have a set of data points $x$ in some
high-dimensional (vector) space.  Then we'd like to find a
$k$-dimensional projector $P$ such that the following error function
is minimized:
\begin{eqnarray}
\sum_x \| x-Px \|^2.
\end{eqnarray}
This error can be rewritten as $\mbox{tr}((I-P)\Sigma)$, where $\Sigma
\equiv \sum_x x x^T$.  And so we simply choose $P$ to project onto the
eigenvectors of $\Sigma$ with the $k$ largest eigenvalues.  The
\emph{principal components} are the eigenvectors of $\Sigma$, in order
of decreasing eigenvalue.  (There may, of course, be some ambiguity
when $\Sigma$ is degenerate).

Practically speaking, suppose we have a billion images, each of which
can be regarded as a vector in a 100,000-dimensional space.  We can
reduce to (say) a 100-dimensional space.  This gets rid of much of the
irrelevant structure, and hopefully leaves a structure that is useful
for comparing images.

\textbf{We don't seem to have much theory of what it means to
  generalize:} We have all these techniques based on
parameter-fitting.  But we have a paucity of strong underlying
theoretical ideas.

\textbf{Deep learning requires nonlinear neurons:} Put another way,
deep learning with linear neurons doesn't help.  Via linear embedding
it's equivalent to a single hidden layer whose size is just the
minimal size of any of the original hidden layers.  So there is
absolutely no advantage to doing deep learning with linear neurons.

\textbf{Tenenbaum, de Silva and Langford, 2000:}
\link{http://scholar.google.ca/scholar?cluster=14602426245887619907&hl=en&as_sdt=0,5}{(link)}
There's a lot I can learn from this paper, so I'll take a lot of
notes.  Some of those notes I'll factor out.  

They mention a technique called multidimensional scaling (MDS), which
I hadn't heard of.  The general idea seems to be that we have a lot of
items, and we know some ``dissimilarities'' between items.  The goal
is to find a metric space embedding of those items so that the
distances are roughly equal to the dissimilarities.

A sample problem: we have a 4096-dimensional space, corresponding to
64 by 64 pixel images.  A (nonlinear) subspace of this corresponds to
images we'd recognize as faces.  How can we characterize this
subspace?  

This is just one possible mathematical formalization of the problem.
In practice, things are more complex.  Our classification will be
fuzzy.  We'll have all kinds of extra contextual information: maybe
we've got an external hint; maybe we can see a nose; maybe the colour
is wrong, but we see enough to suspect it's false colour.  All these
kinds of things are clearly important in how we actually see.  In
other words, we don't just have an algorithm for face detection.  We
have a million related algorithms, and they all affect how well face
detection works.  In some sense you don't solve one problem perfectly.
You solve a network of problems imperfectly --- and then use those
results to improve your performance on the original problem.  It's a
kind of \emph{learning network}.  In a sense this is what a deep
neural network does: it builds up gradually more complicated features.

The algorithm they describe is very simple.  Very roughly (this
certainly contains mistakes): the idea seems to be to take all your
data points and to compute distances between them.  We assume that
when the distances are small, the points are neighbours.  Construct a
graph in which neighbouring points are connected.  Then geodesic
distance is found (approximated) by finding the shortest distance in
the graph.  We then embed the graph in a space of the chosen
dimensionality.  Nice!  Simple, probably pretty easy to implement, and
I expect it lets us find a lot of structure.

It's worth thinking about what the input and output are.  The input to
Iso-map is just a data set --- maybe it's a set of images of a face,
maybe it's a set of words, whatever.  This data lives in a very
high-dimensional space.  What we do is we find an embedding in a much
lower dimensional space --- say, 2-dimensional.  In other words, we're
constructing new features, based on the original features.

\textbf{There are $10^6$ optic nerves and $30,000$ auditory nerves:}
I'm not quite sure what to make of this.  Presumably it means that we
process something like $30$ times as much optical information as
auditory.  I wonder how pixellated the information is?  

\textbf{What happens when we augment the features, with PCA?}  Let's
suppose we start off with 3 features, $x, y, z$.  Then we add $x^2$
and $y^2$ as new features.  Certain subsets of the original space that
weren't linearly approximable \emph{will be} in the new feature space.
This seems like a potentially powerful technique.  What can it be used
to do?  What are its limits?

\textbf{Tenenbaum, Kemp, Griffiths, and Goodman, 2011:}
\link{http://scholar.google.ca/scholar?cluster=2667398573353002097&hl=en&as_sdt=0,5}{(link)}
A review of a particular approach to inductive learning.  They want to
combine Bayesian learning with complex ways of representing knowledge.

Claims that there is strong evidence that children can learn to
generalize their use of words from just a few examples.  This suggests
that there must be some pretty clever underlying patterns to how we
generalize.  ``A massive mismatch looms between information coming in
through our senses and the outputs of cognition''.  

Claims that we humans do reason (implicitly) in Bayesian ways about a
number of things.  Mostly omits the evidence that we \emph{don't} in
some important ways.  This omission bugs me.  They \emph{do} mention
the fact that our conscious assessements of probability tend to be
terrible, which is pleasing.  With that said, I'm not certain about
this --- I just have the strong impression that there are well-known
instances where we certainly don't reason in a Bayesian way.  It'd be
good to have references.

``The biggest remaining obstacle is to understand how structured
symbolic knowledge can be represented in neural circuits.''
Interesting.  I've often wondered exactly this.  They make the
followup comment: ``Connectionist models sidestep these challenges by
denying that brains actually encode such rich knowledge''.  That seems
too strong to me, but there is some truth to it: the connectionists
seem less interested than one might suppose in this question, perhaps
believing that its solution should be deferred.

How would one go about solving this problem?  Actually, what would a
solution / better statement of the problem even look like?  Maybe we
could encode entry-relationships?  In particular, let us suppose we
want to encode $X Y Z$ where $X$ and $Z$ are entities, and $Y$ is the
relationship.  One way of encoding this would be to have a neural
network with nodes for each entity and for each relationship.  We'd
try to design the network so that the only relationships which are
active would be those which are true, given the active entities.

\textbf{Olshausen and Field (1996):} Presents a method for finding
low-complexity representations of natural images, in terms of atomic
images --- which they call ``sparse codes'' --- which are localized,
oriented, and scale-sensitive.  These are found using an unsupervised
learning algorithm with a bias toward good quality, low-complexity
representations.  The codes seem to be quite similar to the receptive
fields found in the human visual system.

The \emph{receptive field} for a cell in the retina is the volume of
space (roughly, a cone) which can stimulate that cell to fire.  Nearby
cells can have overlapping (or nearby) receptive fields.  Other cells
in the visual cortex also have receptive fields, but they may be more
complex, since the light has already been filtered through one or more
levels of processing.  

The paper claims that the receptive fields in the primary visual
cortex are: (a) spatially localized; (b) oriented; and (c) can
distinguish structure at different scales.

There is then a question: so what are those receptive fields?  In a
way, we can view this as being the question: to what type of images do
different cells in our primary visual cortex respond?  Answering that
question seems like a good start for understanding any higher-level
image processing.  It's the question: what are the atoms of image
processing?  Or perhaps a better way is to think of them as the
molecules of image processing, since they're one level up from the
pixel level.

They develop an unsupervised learning algorithm which, trained on
natural data, can find receptive fields that are spatially localized,
oriented, and can distinguish structure at different scales.

Olshausen and Field want to decompose an image as:
\begin{eqnarray}
  I(x,y) = \sum_j a_j \phi_j(x,y).
\end{eqnarray}
The idea is that the $\phi_j$ form a (possibly overcomplete) basis for
the space of images.  They want to choose the $\phi_j$ which ``results
in the coefficient values being as statistically independent as
possible over an ensemble of natural images''.  In some sense, the
different $a_j$ would be ``telling us different things'' about the
image.  They also want the coefficient values to be sparse, favouring
simple representations over more complex.

O \& F try to search for a suitable set of $\phi_j$s by introducing an
error function:
\begin{eqnarray}
  E = -\mbox{[preserve information]}-\lambda\mbox{[sparseness of } a_j {]}.
\end{eqnarray}
This error is \emph{for a single image}.  The first term is just the
$l_2$ error, i.e., (minus) the quadratic distance between the image
and its representation.  The sparseness term is just a nonlinear
function of the $a_j$ coefficients, quantifying how sparse they are.

The idea is to do online learning with this error function, presenting
it with natural images, and gradually minimizing the error.  (I see
later in the article that it was actually batch learning using
conjugate gradient descent.  It appears that some kind of average
error is being computed.)  The result will be an overcomplete basis
set that favours sparse decompositions of images.

The ``sparsification'' idea is a very interesting one.  Basically,
it's a way of trying to force a kind of Occam's razor into the system.
It's a bit like autoencoders, forcing a simple explanation of complex
data.

O \& F note that wavelets have been used to find sparse codes
previously.

\textbf{Restricted Boltzmann machines:} The idea is not to learn a
function, but rather to learn a probability distribution.  There are
two layers of neurons: a visible layer, and a hidden layer.  All
visible units are connected to all hidden units.  The energy of a
given configuration is just:
\begin{eqnarray}
  E(v, h) = -\sum_i a_i v_i-\sum_j b_j h_j-\sum_{ij} w_{ij} v_i h_j \\
  & = & -a \cdot v-b\cdot h -v^T W h,
\end{eqnarray}
where $a$ are the biases for the visible units, $b$ are the biases for
the hidden units, and $W$ is the weight matrix.  The distribution is
just the standard Boltzmann distribution, at some fixed temperature.
Apparently it can be shown that:
\begin{eqnarray}
  p(v_i = 1 | h) = \sigma( a_i + (Wh)i),
\end{eqnarray}
where $\sigma$ is the usual sigmoid function.  (I'll bet this is easy
to show, just by summing out all the other visible units.)
Furthermore, the $v_i$ are independent of one another, given $h$.
This too would be easy to show --- it'll be a straightforward
consequence of the bipartite nature of the graph. So we can compute
the probability of $v$, given $h$, simply by multiplying sigmoids.

Let's suppose we wanted to train an RBM with a set of images.  The
images would correspond to the visible units, while the hidden units
would be feature detectors.  The idea is to adjust the weights and
biases so that training images have a high probability, i.e., a low
energy.  There is a simple learning rule to do this.


\end{document}
