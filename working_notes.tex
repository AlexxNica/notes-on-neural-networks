\documentclass[12pt]{article}

\usepackage{hyperref}

\newcommand{\link}[2]{\href{#1}{#2}}


\begin{document}

\title{Notes on neural networks}
\author{Michael Nielsen\thanks{Email: mn@michaelnielsen.org}$^{,}$\thanks{Web: http://michaelnielsen.org/ddi}}

\maketitle

\textbf{Working notes, by Michael Nielsen:} These are rough working
notes, written as part of my study of neural networks.  Note that they
really are \emph{rough}, and I've made no attempt to clean them up.
There are many misunderstandings, misinterpretations, omissions, and
outright errors in the notes.  I make no apology for this, nor do I
have any interest in cleaning them up. As such, I don't advise reading
the notes, and you certainly shouldn't rely on them!

\textbf{Williams and Zipser (1989) - a gradient-based learning method
  for recurrent neural networks:}
\link{http://scholar.google.ca/scholar?cluster=1352799553544912946\&hl=en\&as\_sdt=0,5}{(link)}
  
  Makes the claim that feedforward networks don't have the "ability to
  store information for later use".  It'd be nice to understand what
  that means.  Obviously there's a trivial sense in which feedforward
  networks can store information based on training data.  Claims that
  backprop requires lots of memory when used with large amounts of
  training data.  I don't understand this claim.
  
  Their model of recurrent neural networks is interesting.  Basically,
  we have a set of neurons, each with an output.  And we have a set of
  inputs to the network.  There is a weight between every pair of
  neurons, and from each input to each neuron.  To compute a neuron's
  output at time $t+1$ we compute the weighted sum of the inputs and
  the outputs at time $t$, and apply the appropriate nonlinear
  function (sigmoid, or whatever).  Note that in order for this
  description to make sense we must specify the behaviour of the
  external inputs over time.  We can incorporate a bias by having an
  external input which is always $1$.
  
  So a recurrent neural network is just like a feedforward network,
  with a weight constraint: the weights in each layer are the same,
  over and over again.  Also, the inputs must be input to every layer
  in the network.
  
  Williams and Zipser take as their supervised training task the goal
  of getting neuron outputs to match certain desired training values
  at certain times.  For instance, you could define a two-neuron
  network that will \emph{eventually} produce the XOR of the inputs.
  
  They define the total error to be the sum over squares of the errors
  in individual neuron outputs.  And we can then do ordinary gradient
  descent with that error function.  They derive a simple dynamical
  system to describe how to improve the weights in the network using
  gradient descent.

  The above algorithm assumes that the weights in the network remain
  constant for all time.  Williams and Zipser then modify the learning
  algorithm, allowing it to change the weights at each step.  The idea
  is simply to compute the total error at any \emph{given} time, and then
  to use gradient descent with that error function to update the
  weights.  (Similar to online learning in feedforward networks.)
  
  Williams and Zipser describe a method of \emph{teacher-forcing},
  modifying the neural network by replacing the output of certain
  neurons by the \emph{desired} output, for training purposes in later
  steps.
  
  Unfortunately, it is still really unclear to me \emph{why} one would
  wish to use recurrent neural networks.  Williams and Zipser describe
  a number of examples, but they don't seem terribly compelling to me.
  
  The algorithm in which the weights can change seems very
  non-physiological to me --- it verges on being an unmotivated
  statistical model.  (I doubt that the weights in the brain swing
  around wildly, but I'll bet that the weights found by this algorithm
  can swing around wildly.)  The algorithm in which the weights are
  fixed seems more biological.
  
  Note that Williams and Zipser \emph{do not} offer any analysis of
  running time for their algorithms, or an understanding of when it is
  likely to work well, and when it is not.  It's very much in the
  empirical let's-see-how-this-works style adopted through much of the
  neural networks literature.
  
  Summing up: the recurrent neural network works by, at each step,
  computing the sigmoid function of the weighted sum of the inputs and
  the previous step's outputs.  Training means specifying a set of
  desired outputs at particular times, and adapting the weights at
  each time-step.  Training works by specifying an error function at
  any given time step, computing the gradient, and updating the
  weights appropriately.

\textbf{Compiling to neural networks:} One idea that may be
  interesting is to create compilers which would translate a program
  written in a conventional programming language into a neural
  network.  I'd be especially interested in seeing how this works for
  AI workhorses such as Prolog.  What could we learn from such a
  procedure?  (1) Perhaps we could figure out how to link up multiple
  neural modules, with one or more of the modules coming from the
  compiler? (2) Maybe we could use a learning technique to further
  improve the performance of the compiled network.  Googling doesn't
  reveal a whole lot, although I did find a paper by
  \link{http://scholar.google.ca/scholar?cluster=10518384657895134615\&hl=en\&as\_sdt=0,5}{Thrun}
  where he discusses decompiling, i.e., extracting rules from a neural
  network.  Thrun uses a technique he calls validity-interval
  analysis, basically propagating intervals for inputs and outputs
  forwards and backwards through a network.

\textbf{Softmax function:} Suppose $q_j$ is some set of values.  Then
  we define the softmax function by:

$$p_j \equiv \exp(q_j)/\sum_k \exp(q_j).$$
  
This is a probability distribution, which preserves the order of the
original values.  You can, for example, take the softmax in the final
layer of a neural network, taking the weighted sum of inputs as the
$q_j$ values, and then applying the softmax.  The output from the
network can then be interpreted as a probability distribution.

\textbf{Baldi and Hornik, 1989 - characterizes linear autoencoders}:
\link{http://scholar.google.ca/scholar?cluster=11637720331851320383&hl=en&as_sdt=0,5}{(link)}
We have a three-layer network, and the output is related to the input
by $x \rightarrow ABx$, where $B$ describes the first layer of
weights, and $A$ the second layer.  The goal is to find weight
matrices $A$ and $B$ to minimize:
\begin{eqnarray}
\sum_x \|x-ABx\|^2.
\end{eqnarray}
The challenge is that the hidden layer has a \emph{smaller} number $h$
of neurons than the input layer (which is, of course, of the same size
as the output layer)\footnote{It's not quite clear to me what $h$
  should parameterize.  I'll use it to parameterize the number of
  dimensions in the vector space representing outputs from the hidden
  units.  It seems likely that it'd be better to write $2^h$, but I'll
  ignore that.}.  Let me try an attack on this without reading the
paper.  That sum above is just:
\begin{eqnarray}
\mbox{tr}((I-AB)^2 \Sigma),
\end{eqnarray}
where $\Sigma \equiv \sum_x x x^T$.  To minimize this what we want to
do is obvious (and easily proven): we'll choose $A$ and $B$ so that
$AB$ is a $h$-dimensional projector onto the span of the eignenvectors
of $\Sigma$ with the $h$ largest eigenvalues.  Let $P(\Sigma, h)$
denote such a projector, so:
\begin{eqnarray}
AB = P(\Sigma, h).
\end{eqnarray}
We can easily characterize such $A$ and $B$.  $A$ should take the
space $P(\Sigma, h)$ into the space spanned by the outputs from the
hidden units, and $B$ should then undo that transformation.  There is
an orthogonal freedom inbetween time, and a possible freedom in
$P(\Sigma, h)$.  This completely characterizes $A$ and $B$.

Summing up, in a linear neural network, \emph{a linear autoencoder is
  just doing principal components analysis}.  So \emph{a non-linear
  autoencoder can be thought of as a non-linear generalization of
  PCA}.  That's a useful fact to remember.  Examination of the
remainder of the paper suggests that these are the key facts.

\textbf{Thinking geometrically:} Suppose we're asked to tell the
difference between pictures of a human face, and pictures of a
giraffe.  We can represent the pictures as points $x$ in a very
high-dimensional space.  And so our task is to divide that space up
into two parts: one is classified as giraffe, the other as human face.
(Maybe it should be three parts: the thrid part would be: neither face
nor giraffe).  And so what we really want is algorithms for dividing
up that space.  In some sense we're interested in understanding the
space of all such algorithms. 

It'd be interesting to lay out all the different curlicues to thinking
in this way: the opportunities, and the pitfalls.  There are at least
three broad approaches: (1) the \emph{pure geometric approach}, based
on finding mathematical structures to divide the space; (2) the
\emph{biological approach}, where we try to figure out how we do it;
and (3) the \emph{kludge approach}, where we simply try lots of ideas,
and pile them up on top of one another.  That's a pretty rough
division, but seems like a good starting point for thought.  My bet is
that progress comes from playing these ideas off against one another.

\textbf{Finding linear approximations (PCA):} It'll be useful to
review PCA here.  Suppose we have a set of data points $x$ in some
high-dimensional (vector) space.  Then we'd like to find a
$k$-dimensional projector $P$ such that the following error function
is minimized:
\begin{eqnarray}
\sum_x \| x-Px \|^2.
\end{eqnarray}
This error can be rewritten as $\mbox{tr}((I-P)\Sigma)$, where $\Sigma
\equiv \sum_x x x^T$.  And so we simply choose $P$ to project onto the
eigenvectors of $\Sigma$ with the $k$ largest eigenvalues.  The
\emph{principal components} are the eigenvectors of $\Sigma$, in order
of decreasing eigenvalue.  (There may, of course, be some ambiguity
when $\Sigma$ is degenerate).

Practically speaking, suppose we have a billion images, each of which
can be regarded as a vector in a 100,000-dimensional space.  We can
reduce to (say) a 100-dimensional space.  This gets rid of much of the
irrelevant structure, and hopefully leaves a structure that is useful
for comparing images.

\textbf{We don't seem to have much theory of what it means to
  generalize:} We have all these techniques based on
parameter-fitting.  But we have a paucity of strong underlying
theoretical ideas.

\textbf{Deep learning requires nonlinear neurons:} Put another way,
deep learning with linear neurons doesn't help.  Via linear embedding
it's equivalent to a single hidden layer whose size is just the
minimal size of any of the original hidden layers.  So there is
absolutely no advantage to doing deep learning with linear neurons.

\textbf{Tenenbaum, de Silva and Langford, 2000:}
\link{http://scholar.google.ca/scholar?cluster=14602426245887619907&hl=en&as_sdt=0,5}{(link)}
There's a lot I can learn from this paper, so I'll take a lot of
notes.  Some of those notes I'll factor out.  

They mention a technique called multidimensional scaling (MDS), which
I hadn't heard of.  The general idea seems to be that we have a lot of
items, and we know some ``dissimilarities'' between items.  The goal
is to find a metric space embedding of those items so that the
distances are roughly equal to the dissimilarities.

A sample problem: we have a 4096-dimensional space, corresponding to
64 by 64 pixel images.  A (nonlinear) subspace of this corresponds to
images we'd recognize as faces.  How can we characterize this
subspace?  

This is just one possible mathematical formalization of the problem.
In practice, things are more complex.  Our classification will be
fuzzy.  We'll have all kinds of extra contextual information: maybe
we've got an external hint; maybe we can see a nose; maybe the colour
is wrong, but we see enough to suspect it's false colour.  All these
kinds of things are clearly important in how we actually see.  In
other words, we don't just have an algorithm for face detection.  We
have a million related algorithms, and they all affect how well face
detection works.  In some sense you don't solve one problem perfectly.
You solve a network of problems imperfectly --- and then use those
results to improve your performance on the original problem.  It's a
kind of \emph{learning network}.  In a sense this is what a deep
neural network does: it builds up gradually more complicated features.

The algorithm they describe is very simple.  Very roughly (this
certainly contains mistakes): the idea seems to be to take all your
data points and to compute distances between them.  We assume that
when the distances are small, the points are neighbours.  Construct a
graph in which neighbouring points are connected.  Then geodesic
distance is found (approximated) by finding the shortest distance in
the graph.  We then embed the graph in a space of the chosen
dimensionality.  Nice!  Simple, probably pretty easy to implement, and
I expect it lets us find a lot of structure.

\textbf{There are $10^6$ optic nerves and $30,000$ auditory nerves:}
I'm not quite sure what to make of this.  Presumably it means that we
process something like $30$ times as much optical information as
auditory.  I wonder how pixellated the information is?  

\textbf{What happens when we augment the features, with PCA?}  Let's
suppose we start off with 3 features, $x, y, z$.  Then we add $x^2$
and $y^2$ as new features.  Certain subsets of the original space that
weren't linearly approximable \emph{will be} in the new feature space.
This seems like a potentially powerful technique.  What can it be used
to do?  What are its limits?

\end{document}
